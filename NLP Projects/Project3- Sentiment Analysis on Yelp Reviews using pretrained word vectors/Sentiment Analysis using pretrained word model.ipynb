{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Sentiment Analysis using pretrained word model\n"],"metadata":{"id":"UuPdnyWXh7vb"}},{"cell_type":"markdown","source":["**IMPORTANT**<br>\n","Enable **GPU acceleration** by going to *Runtime > Change Runtime Type*. Keep in mind that, on certain tiers, you're not guaranteed GPU access depending on usage history and current load.\n","<br><br>"],"metadata":{"id":"91LZr9ogtHi_"}},{"cell_type":"markdown","source":["To demonstrate word vectors, we are going to use **Gensim**."],"metadata":{"id":"qOE58pxDZqah"}},{"cell_type":"code","source":["# Upgrade gensim just in case.\n","!pip install -U gensim==4.*"],"metadata":{"id":"2vNBSTzIKkRT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import collections\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import spacy\n","import tensorflow as tf\n","\n","from gensim.models.keyedvectors import KeyedVectors\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split\n","from tensorflow import keras\n","from tensorflow.keras import layers"],"metadata":{"id":"kIMAMpBaoqkP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**NOTE**<br>\n","Gensim library:<br>\n","https://radimrehurek.com/gensim/models/word2vec.html<br>\n","https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\n","\n"],"metadata":{"id":"SBQlbyiVY3WJ"}},{"cell_type":"markdown","source":["## Using Pretrained, Third-Party Vectors"],"metadata":{"id":"A_rgKP1vJ2kT"}},{"cell_type":"markdown","source":["There are a variety of pretrained, static word vector packages. We will use the **Google News** vectors, a collection of three million, 300-dimension word vectors trained from three billion words from a Google News corpus (circa 2015)."],"metadata":{"id":"RHMDs5FPKXUx"}},{"cell_type":"markdown","source":["We'll need to first download the actual word vectors. It's over a gigabyte in size but will fit within our environment."],"metadata":{"id":"lQEAXBoBMidI"}},{"cell_type":"code","source":["!gdown \"https://drive.google.com/uc?id=1BpfbHu4denceXiv8yfdY3EHgjKIcULku\""],"metadata":{"id":"dIKSA-KMbj8E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embedding_file = './GoogleNews-vectors-negative300.bin.gz'\n","embedding_file"],"metadata":{"id":"6FtVSlPQbj4F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we'll have **gensim** load the vectors through the **KeyedVectors** module which will enable us to look up vectors by tokens and indices.<br>\n","https://radimrehurek.com/gensim/models/keyedvectors.html"],"metadata":{"id":"BXQRngPaPWhE"}},{"cell_type":"markdown","source":["We'll train a **Keras** model to use these Google News vectors to perform sentiment analysis on a bunch of **Yelp** reviews.\n","<br><br>\n","For the model, we will use 1,000,000 word vectors.\n","\n"],"metadata":{"id":"Am-9bvppYKuS"}},{"cell_type":"code","source":["%%time\n","word_vectors = KeyedVectors.load_word2vec_format(embedding_file, binary=True, limit=1000000)\n","word_vectors"],"metadata":{"id":"VVLj4fSMzvHJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The dataset we'll use is *Yelp Polarity Reviews*, a collection of ~600,000 reviews for both training and testing.<br><br>\n","The original Yelp reviews use a five-star rating system. The ratings in this dataset have been modified to simply be negative (label==1) or positive (label==2).<br>\n","https://www.tensorflow.org/datasets/catalog/yelp_polarity_reviews<br><br>\n","Tensorflow comes with a datasets loader but we're going to download the file manually and process the data ourselves for completeness."],"metadata":{"id":"vdZQNavVYxfP"}},{"cell_type":"code","source":["!wget -P /root/input/ -c \"https://s3.amazonaws.com/fast-ai-nlp/yelp_review_polarity_csv.tgz\""],"metadata":{"id":"RcMEE11bzz_j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Unzipping the archive results in *train.csv* and *test.csv* files placed in the default *contents* folder of our environment."],"metadata":{"id":"wnTnApF4aq7k"}},{"cell_type":"code","source":["!tar xvzf /root/input/yelp_review_polarity_csv.tgz\n","\n","# Show current working directory.\n","!pwd"],"metadata":{"id":"-UK1y9kjzz7y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The **Pandas** library makes it simple to load a CSV file into memory and manipulate the data.<br>\n","https://pandas.pydata.org/<br>\n","https://pandas.pydata.org/docs/<br>\n","https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html?highlight=read_csv"],"metadata":{"id":"OaJCSM3lbK-E"}},{"cell_type":"markdown","source":["Here, we're loading the CSV into a Pandas **dataframe** (sort of like an in-memory table) and explicitly naming the columns.<br>\n","https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html?highlight=dataframe#pandas.DataFrame"],"metadata":{"id":"xIG4UVMkbq9E"}},{"cell_type":"code","source":["yelp_train = pd.read_csv('yelp_review_polarity_csv/train.csv', names=['sentiment', 'review'])\n","print(yelp_train.shape)"],"metadata":{"id":"tdLoK1Jozz0Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can get a quick view of the data through the *head* method.<br>\n","https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html"],"metadata":{"id":"lGgJl1QCb738"}},{"cell_type":"code","source":["yelp_train.head()"],"metadata":{"id":"jVHtVqOBzzwS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To save on training time, we'll train on 100,000 reviews rather than the full set. To do that, we'll shuffle the dataset using the *sample* method and *copy* the first 100,000 entries. The reason to shuffle first is to ensure we get a mix of reviews from a variety of businesses (in case the data is sorted in some way).<br>\n","https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html<br>\n","https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.copy.html\n"],"metadata":{"id":"9wS0o4gEclCT"}},{"cell_type":"code","source":["TRAIN_SIZE = 100000\n","yelp_train = yelp_train.sample(frac=1, random_state=1)[:TRAIN_SIZE].copy()\n","print(yelp_train.shape)"],"metadata":{"id":"YacSH4F0c5aU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The next thing to do is adjust the labels. This is a **binary classification problem**, so our model's output layer will be a single unit with a **sigmoid** activation function. This function's output will be between 0 and 1 which is then compared against the training label. But the labels are currently 1 for negative, and 2 for positive, which is going to cause problems when calculating the loss.<br><br>\n","So we'll simply replace the 1s with 0s, and 2s with 1s using the *replace* method.<br>\n","https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html\n","<br><br>\n","Alternatively, we could keep the labels as-is and treat this as a **multiclassification** problem with two labels and use a **softmax**, but we would then need to **one-hot encode** the labels.\n"],"metadata":{"id":"DLraxnTMd9iE"}},{"cell_type":"code","source":["yelp_train['sentiment'].replace(to_replace=1, value=0, inplace=True)\n","yelp_train['sentiment'].replace(to_replace=2, value=1, inplace=True)"],"metadata":{"id":"kbmYKm-SjGnm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["yelp_train.head()"],"metadata":{"id":"LlzWLECijrg_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we'll create train/validation splits.<br>\n","https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"],"metadata":{"id":"rILnYdQMgIlh"}},{"cell_type":"code","source":["yelp_train_split, yelp_val_split = train_test_split(yelp_train, train_size=0.85, random_state=1)\n","print(yelp_train_split.shape)\n","print(yelp_val_split.shape)"],"metadata":{"id":"CC-VSNcn7Cpq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set up training data.\n","train_reviews = yelp_train_split['review']\n","y_train = np.array(yelp_train_split['sentiment'])\n","print(y_train.shape)\n","print(train_reviews.shape)"],"metadata":{"id":"9Xj4WyLb7Wdr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_train"],"metadata":{"id":"QuqgNKQikpk8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_reviews[:2]"],"metadata":{"id":"gObJ5slJkxi8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set up validation data.\n","val_reviews = yelp_val_split['review']\n","y_val = np.array(yelp_val_split['sentiment'])\n","print(y_val.shape)\n","print(val_reviews.shape)"],"metadata":{"id":"6li9RNFGkcis"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["val_reviews[:2]"],"metadata":{"id":"qyGAiu4bk4Lv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A quick sanity check to see how our data is distributed (e.g. balanced or skewed)."],"metadata":{"id":"CK9V0dJAgbSJ"}},{"cell_type":"code","source":["collections.Counter(y_train)"],"metadata":{"id":"MHBWB2vc7ie1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Because we're relying more on richer encodings (in this case, word vectors), we won't perform as much preprocessing this time around. We'll stick with using the regular Keras **tokenizer** and just filter out numbers and certain symbols.<br>\n","https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer<br><br>\n","We'll also have the tokenizer limit itself to tokenizing only the most frequent 20,000 words. This way, the model will focus on the most frequent descriptive sentiment words."],"metadata":{"id":"M0fM5vRygqfq"}},{"cell_type":"code","source":["tokenizer = keras.preprocessing.text.Tokenizer(num_words=20000,\n","                                               filters='0123456789!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',\n","                                               lower=True)"],"metadata":{"id":"UI4FgHPa4p2T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Build the vocabulary.\n","tokenizer.fit_on_texts(train_reviews)"],"metadata":{"id":"ayGLszCqzzoK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The next step is to vectorize our reviews.\n","\n","We are going to use the *text_to_sequences* method to turn each review into a sequence of integers, with each integer representing its corresponding token.<br>\n","https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#texts_to_sequences\n","\n"],"metadata":{"id":"v5zDD6RCBzxH"}},{"cell_type":"code","source":["%%time\n","X_train = tokenizer.texts_to_sequences(train_reviews)"],"metadata":{"id":"OAd431jp6Ydg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The first review in the training set, vectorized.\n","print(X_train[0])"],"metadata":{"id":"PPo0mNRB6YZL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can look up the corresponding tokens using the tokenizer's *index_word* dict. Here are the tokens corresponding to the first three integers from the first vectorized review."],"metadata":{"id":"kH4I4SeoqVDn"}},{"cell_type":"code","source":["[tokenizer.index_word[x] for x in X_train[0][:3]]"],"metadata":{"id":"jkOI7jYFqb44"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can also convert the integer sequence back to text using the *sequences_to_texts* method, and compare it against the original text.<br>\n","https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#sequences_to_texts"],"metadata":{"id":"ahgfyZhDrHDJ"}},{"cell_type":"code","source":["# Review excerpt reconstructed from integer sequence.\n","tokenizer.sequences_to_texts([X_train[0]])[0][:300]"],"metadata":{"id":"RPlvLdz8rGZZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Original review text.\n","train_reviews.iloc[0][:300]"],"metadata":{"id":"ZirHR4_OC87H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Some models and situations require us to **pad** our sequences to the same length. While that's not the case here, it can still be beneficial to have all our inputs (and consequently, our batches) to be of uniform size to help with optimizations.<br><br>\n","In this case, we'll make all our reviews 200 tokens in length (in practice, you can choose a number based on some analysis). So the reviews longer than 200 tokens will be truncated, while the reviews shorter than 200 will be padded with zeroes.<br>\n","https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences"],"metadata":{"id":"bEqDNjactNk1"}},{"cell_type":"code","source":["MAX_REVIEW_LEN = 200\n","X_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=MAX_REVIEW_LEN)"],"metadata":{"id":"2lTEjMgDqIPp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" # truncated example\n"," print(X_train[0])"],"metadata":{"id":"d2tHPfAVv37k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# padded example\n","print(X_train[1])"],"metadata":{"id":"vdoZXJsOmTMw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Our training set is prepared. We can now also vectorize and pad our validation set."],"metadata":{"id":"nBAg3laKwDWO"}},{"cell_type":"code","source":["X_val = tokenizer.texts_to_sequences(val_reviews)\n","X_val = keras.preprocessing.sequence.pad_sequences(X_val, maxlen=MAX_REVIEW_LEN)"],"metadata":{"id":"E9P72JJYHqKp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we need to incorporate the Google News vectors (currently loaded into gensim) into our Keras model. What we'll do is create an embedding matrix that maps each tokenizer integer to its respective word vector.<br><br>\n","For example, here's the index for the word \"good\" from the Keras tokenizer and the word vector for \"good\" from gensim. We want a matrix which maps the index to the vector.\n"],"metadata":{"id":"P8dy9OWMwafM"}},{"cell_type":"code","source":["print(tokenizer.word_index['good'])"],"metadata":{"id":"5m7OW6-pwaJc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Part of the vector for the word 'good'.\n","print(word_vectors['good'][:50])"],"metadata":{"id":"3rOAy_IkHqfx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We'll create this embedding matrix by first initializing a matrix of zeros, then looping over every word in the tokenizer vocabulary and:\n","1. Checking if the word has a corresponding vector in gensim.\n","2. If it does, then copy the vector into the matrix row corresponding to the word's index."],"metadata":{"id":"elzisAtxyO-o"}},{"cell_type":"code","source":["# + 1 to account for padding token.\n","num_tokens = len(tokenizer.word_index) + 1\n","\n","# Initialize a matrix of zeroes of size: vocabulary x embedding dimension.\n","embedding_dim = 300\n","embedding_matrix = np.zeros((num_tokens, embedding_dim))\n","\n","for word, i in tokenizer.word_index.items():\n","  if word_vectors.has_index_for(word):\n","    embedding_matrix[i] = word_vectors[word].copy()\n"],"metadata":{"id":"Iw25Irs4J-JG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Quick visual check.\n","print(embedding_matrix[tokenizer.word_index['good']][:50])"],"metadata":{"id":"uncvu4lcJ-OI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We're ready to build our first model using pretrained word vectors. The first layer we'll add is a Keras **embedding** layer which is essentially a trainable lookup table/matrix.<br>\n","https://keras.io/api/layers/base_layer/#layer-class<br>\n","https://keras.io/api/layers/core_layers/embedding/<br><br>\n","In this case, we'll populate the **embedding** layer with the embedding matrix we created, and set *trainable* to True. This means we'll allow the learning algorithm training the classification model to adjust/fine-tune the word vectors as needed for greater performance. This corresponds to one of the scenarios we covered in the slides."],"metadata":{"id":"CmzNcyFu0Noo"}},{"cell_type":"code","source":["embedding_layer = layers.Embedding(\n","    num_tokens,\n","    embedding_dim,\n","    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n","    input_length=MAX_REVIEW_LEN,\n","    trainable=True\n",")"],"metadata":{"id":"ALs6rkTnJ-aj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We'll use a simple architecture for this model. Each training example is a sequence of *integers* which gets converted to a sequence of *vectors* (embeddings), but subsequent layers are expecting one vector per review. So we're inserting a **GlobalAveragePooling1D** layer after the embedding layer to average out all the word vectors into a single vector, before sending it further into the network. For classification, this can be pretty effective as a base model approach.<br>\n","https://keras.io/api/layers/pooling_layers/global_average_pooling1d/<br>\n","\n","There was no science behind choosing 128 units in the first hidden layer and 64 units in the second hidden layer. The intuition was that the signal would be distilled from 300 dimensions down to 128 dimensions, then down to 64 dimensions before going to output.\n","\n"],"metadata":{"id":"OqRgEWpb2iuI"}},{"cell_type":"code","source":["tf.random.set_seed(0)\n","\n","model = keras.Sequential()\n","\n","# This layer will output a sequence of 300-dimension *vectors*, one for each element in the input sequence.\n","model.add(embedding_layer)\n","\n","# This layer will calculate an average of those vectors.\n","model.add(layers.GlobalAveragePooling1D())\n","\n","model.add(layers.Dense(128, activation='relu', kernel_initializer=tf.keras.initializers.random_normal(seed=1)))\n","model.add(layers.Dense(64, activation='relu', kernel_initializer=tf.keras.initializers.random_normal(seed=1)))\n","model.add(layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.random_normal(seed=1)))\n","\n","model.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])"],"metadata":{"id":"1RrLd4gzbhVS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here's an example of what's going to happen under the hood to turn review text into a single vector for the dense layers:"],"metadata":{"id":"dlmis9Oz4_jy"}},{"cell_type":"code","source":["review = \"fantastic papaya steak\"\n","print(f\"Review: {review}\")\n","\n","review_sequence = tokenizer.texts_to_sequences([review])\n","print(f\"Review as sequence of integers: {review_sequence}\")\n","\n","review_embeddings = embedding_layer(np.array(review_sequence))\n","print(f\"Review embeddings shape: (Batch size: {review_embeddings.shape[0]}, \\\n","Sequence length: {review_embeddings.shape[1]}, \\\n","Embedding size: {review_embeddings.shape[2]})\")\n","\n","# How our document will be presented to the rest of the neural network.\n","print(f\"Average of embeddings (shape): {np.mean(review_embeddings, axis=1).shape}\")"],"metadata":{"id":"1r5K-Qqb1d-H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["When we call the model's *summary* method, note how there are no params for the **GlobalAveragePooling1D** layer."],"metadata":{"id":"546ouDEv41Ea"}},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"Pz31QQcb5nq5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We won't use **early stopping** for this run. This way, we'll be able to compare metrics between the train and validation sets."],"metadata":{"id":"AwGYMD028zhO"}},{"cell_type":"code","source":["history = model.fit(X_train, y_train, epochs=20, batch_size=512, validation_data=(X_val, y_val))"],"metadata":{"id":"oU6QBey79cis"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_train_vs_val_performance(history):\n","  training_losses = history.history['loss']\n","  validation_losses = history.history['val_loss']\n","\n","  training_accuracy = history.history['accuracy']\n","  validation_accuracy = history.history['val_accuracy']\n","\n","  epochs = range(1, len(training_losses) + 1)\n","\n","  import matplotlib.pyplot as plt\n","  fig, (ax1, ax2) = plt.subplots(2)\n","  fig.set_figheight(15)\n","  fig.set_figwidth(15)\n","  fig.tight_layout(pad=5.0)\n","\n","  # Plot training vs. validation loss.\n","  ax1.plot(epochs, training_losses, 'bo', label='Training Loss')\n","  ax1.plot(epochs, validation_losses, 'b', label='Validation Loss')\n","  ax1.title.set_text('Training vs. Validation Loss')\n","  ax1.set_xlabel('Epoch')\n","  ax1.set_ylabel('Loss')\n","  ax1.legend()\n","\n","  # PLot training vs. validation accuracy.\n","  ax2.plot(epochs, training_accuracy, 'bo', label='Training Accuracy')\n","  ax2.plot(epochs, validation_accuracy, 'b', label='Validation Accuracy')\n","  ax2.title.set_text('Training vs. Validation Accuracy')\n","  ax2.set_xlabel('Epoch')\n","  ax2.set_ylabel('Accuracy')\n","  ax2.legend()\n","\n","  plt.show()"],"metadata":{"id":"Neard5tEGwnG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_train_vs_val_performance(history)"],"metadata":{"id":"lbSk2qE1cEfS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's train the model again using above information.\n","We'll initialize a new embedding layer and model and train for epochs equalling the point where we saw the validation loss diverge from the training loss.<br>\n","\n","**NOTE**: We need to initialize a new embedding layer here because we set the *learnable* parameter to **True** in the previous embedding layer. This means the previous embeddings were almost certainly updated by the learning algorithm. So we're re-training a new model now with the original embeddings."],"metadata":{"id":"xdeEdLD6W91U"}},{"cell_type":"code","source":["embedding_layer = layers.Embedding(\n","    num_tokens,\n","    embedding_dim,\n","    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n","    input_length=MAX_REVIEW_LEN,\n","    trainable=True\n",")\n","\n","model = keras.Sequential()\n","model.add(embedding_layer)\n","model.add(layers.GlobalAveragePooling1D())\n","model.add(layers.Dense(128, activation='relu'))\n","model.add(layers.Dense(64, activation='relu'))\n","model.add(layers.Dense(1, activation='sigmoid'))\n","\n","model.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","model.fit(X_train, y_train, epochs=3, batch_size=512, validation_data=(X_val, y_val))"],"metadata":{"id":"V_nWiYy6cEbJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now that we have a trained model, let's try it on the test data. As we did with the training data, we'll:\n","1. Replace the labels with 0 for negative sentiment, and 1 for positive.\n","2. Convert the reviews into a sequence of integers and pad/truncate each review to a fixed length."],"metadata":{"id":"_4X_sdScYPAP"}},{"cell_type":"code","source":["yelp_test = pd.read_csv('yelp_review_polarity_csv/test.csv', names=['sentiment', 'review'])"],"metadata":{"id":"AUsripS7SZdk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["yelp_test['sentiment'].replace(to_replace=1, value=0, inplace=True)\n","yelp_test['sentiment'].replace(to_replace=2, value=1, inplace=True)\n","yelp_test.head()"],"metadata":{"id":"bm4Day_jYbTe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_test = np.array(yelp_test['sentiment'])\n","print(y_test)"],"metadata":{"id":"Qks0uXyzYbTe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_test = tokenizer.texts_to_sequences(yelp_test['review'])"],"metadata":{"id":"-UyBts6EYbTe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=MAX_REVIEW_LEN)"],"metadata":{"id":"loh2rh3WYbTe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.evaluate(X_test, y_test)"],"metadata":{"id":"yKmqmLUkYbTf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Not bad for a conceptually simple model where we average out a review's word vectors, run it through a few plain hidden layers, and out through a sigmoid function with no regularization and just using defaults for model components (e.g. optimizer settings).<br><br>\n","We can now use the model for predictions."],"metadata":{"id":"9GBMe-YKZCDP"}},{"cell_type":"code","source":["def sentiment(reviews):\n","  seqs = tokenizer.texts_to_sequences(reviews)\n","  seqs = keras.preprocessing.sequence.pad_sequences(seqs, maxlen=MAX_REVIEW_LEN)\n","  return model.predict(seqs)\n"],"metadata":{"id":"rNbNyr8PVJMv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Real reviews from Google Reviews.\n","pos_review = \"The best seafood joint in East Village San Diego!  Great lobster roll, great fish, great oysters, great bread, great cocktails, and such amazing service.  The atmosphere is top notch and the location is so much fun being located just a block away from Petco Park (San Diego Padres Stadium).\"\n","neg_review = \"A thoroughly disappointing experience. When you book a Marriott you expect a certain standard. Albany falls way short. Room cleaning has to be booked 24 hours in advance but nobody thought to mention this at check in. The hotel is tired and needs a face-lift. The only bright light in a sea of mediocrity were the pancakes at breakfast. Sadly they weren't enough to save the experience. If you travel to Albany, then do yourself a big favour and book the Westin.\""],"metadata":{"id":"Jy-4MWV1VJRA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(sentiment([pos_review, neg_review]))"],"metadata":{"id":"wH8b-Z3GVJU3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We get right predictions for the reviews."],"metadata":{"id":"7tRSU3y3pWTw"}}]}