{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Sentiment Analysis using pretrained word models\n"],"metadata":{"id":"UuPdnyWXh7vb"}},{"cell_type":"markdown","source":["**IMPORTANT**<br>\n","Enable **GPU acceleration** by going to *Runtime > Change Runtime Type*. Keep in mind that, on certain tiers, you're not guaranteed GPU access depending on usage history and current load.\n","<br><br>"],"metadata":{"id":"91LZr9ogtHi_"}},{"cell_type":"markdown","source":["To demonstrate word vectors, we are going to use **Gensim**."],"metadata":{"id":"qOE58pxDZqah"}},{"cell_type":"code","source":["# Upgrade gensim just in case.\n","!pip install -U gensim==4.*"],"metadata":{"id":"2vNBSTzIKkRT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import collections\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import spacy\n","import tensorflow as tf\n","\n","from gensim.models.keyedvectors import KeyedVectors\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split\n","from tensorflow import keras\n","from tensorflow.keras import layers"],"metadata":{"id":"kIMAMpBaoqkP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training New Embeddings and a Model at the Same Time"],"metadata":{"id":"A_rgKP1vJ2kT"}},{"cell_type":"markdown","source":["The dataset we'll use is *Yelp Polarity Reviews*, a collection of ~600,000 reviews for both training and testing.<br><br>\n","The original Yelp reviews use a five-star rating system. The ratings in this dataset have been modified to simply be negative (label==1) or positive (label==2).<br>\n","https://www.tensorflow.org/datasets/catalog/yelp_polarity_reviews<br><br>\n","Tensorflow comes with a datasets loader but we're going to download the file manually and process the data ourselves for completeness."],"metadata":{"id":"vdZQNavVYxfP"}},{"cell_type":"code","source":["!wget -P /root/input/ -c \"https://s3.amazonaws.com/fast-ai-nlp/yelp_review_polarity_csv.tgz\""],"metadata":{"id":"RcMEE11bzz_j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Unzipping the archive results in *train.csv* and *test.csv* files placed in the default *contents* folder of our environment."],"metadata":{"id":"wnTnApF4aq7k"}},{"cell_type":"code","source":["!tar xvzf /root/input/yelp_review_polarity_csv.tgz\n","\n","# Show current working directory.\n","!pwd"],"metadata":{"id":"-UK1y9kjzz7y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The **Pandas** library makes it simple to load a CSV file into memory and manipulate the data.<br>\n","https://pandas.pydata.org/<br>\n","https://pandas.pydata.org/docs/<br>\n","https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html?highlight=read_csv"],"metadata":{"id":"OaJCSM3lbK-E"}},{"cell_type":"markdown","source":["Here, we're loading the CSV into a Pandas **dataframe** (sort of like an in-memory table) and explicitly naming the columns.<br>\n","https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html?highlight=dataframe#pandas.DataFrame"],"metadata":{"id":"xIG4UVMkbq9E"}},{"cell_type":"code","source":["yelp_train = pd.read_csv('yelp_review_polarity_csv/train.csv', names=['sentiment', 'review'])\n","print(yelp_train.shape)"],"metadata":{"id":"tdLoK1Jozz0Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can get a quick view of the data through the *head* method.<br>\n","https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html"],"metadata":{"id":"lGgJl1QCb738"}},{"cell_type":"code","source":["yelp_train.head()"],"metadata":{"id":"jVHtVqOBzzwS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To save on training time, we'll train on 100,000 reviews rather than the full set. To do that, we'll shuffle the dataset using the *sample* method and *copy* the first 100,000 entries. The reason to shuffle first is to ensure we get a mix of reviews from a variety of businesses (in case the data is sorted in some way).<br>\n","https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html<br>\n","https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.copy.html\n"],"metadata":{"id":"9wS0o4gEclCT"}},{"cell_type":"code","source":["TRAIN_SIZE = 100000\n","yelp_train = yelp_train.sample(frac=1, random_state=1)[:TRAIN_SIZE].copy()\n","print(yelp_train.shape)"],"metadata":{"id":"YacSH4F0c5aU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The next thing to do is adjust the labels. This is a **binary classification problem**, so our model's output layer will be a single unit with a **sigmoid** activation function. This function's output will be between 0 and 1 which is then compared against the training label. But the labels are currently 1 for negative, and 2 for positive, which is going to cause problems when calculating the loss.<br><br>\n","So we'll simply replace the 1s with 0s, and 2s with 1s using the *replace* method.<br>\n","https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html\n","<br><br>\n","Alternatively, we could keep the labels as-is and treat this as a **multiclassification** problem with two labels and use a **softmax**, but we would then need to **one-hot encode** the labels (at least based on what we've learnt so far).\n"],"metadata":{"id":"DLraxnTMd9iE"}},{"cell_type":"code","source":["yelp_train['sentiment'].replace(to_replace=1, value=0, inplace=True)\n","yelp_train['sentiment'].replace(to_replace=2, value=1, inplace=True)"],"metadata":{"id":"kbmYKm-SjGnm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["yelp_train.head()"],"metadata":{"id":"LlzWLECijrg_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As we've done throughout this course, we'll create train/validation splits.<br>\n","https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"],"metadata":{"id":"rILnYdQMgIlh"}},{"cell_type":"code","source":["yelp_train_split, yelp_val_split = train_test_split(yelp_train, train_size=0.85, random_state=1)"],"metadata":{"id":"CC-VSNcn7Cpq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set up training data.\n","train_reviews = yelp_train_split['review']\n","y_train = np.array(yelp_train_split['sentiment'])\n","\n","# Set up validation data.\n","val_reviews = yelp_val_split['review']\n","y_val = np.array(yelp_val_split['sentiment'])"],"metadata":{"id":"9Xj4WyLb7Wdr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A quick sanity check to see how our data is distributed (e.g. balanced or skewed)."],"metadata":{"id":"CK9V0dJAgbSJ"}},{"cell_type":"code","source":["collections.Counter(y_train)"],"metadata":{"id":"MHBWB2vc7ie1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Because we're relying more on richer encodings (in this case, word vectors), we won't perform as much preprocessing this time around. We'll stick with using the regular Keras **tokenizer** and just filter out numbers and certain symbols.<br>\n","https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer<br><br>\n","We'll also have the tokenizer limit itself to tokenizing only the most frequent 20,000 words. This way, the model will focus on the most frequent descriptive sentiment words."],"metadata":{"id":"M0fM5vRygqfq"}},{"cell_type":"code","source":["tokenizer = keras.preprocessing.text.Tokenizer(num_words=20000,\n","                                               filters='0123456789!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',\n","                                               lower=True)"],"metadata":{"id":"UI4FgHPa4p2T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Build the vocabulary."],"metadata":{"id":"gXQXX82chivF"}},{"cell_type":"code","source":["%%time\n","tokenizer.fit_on_texts(train_reviews)"],"metadata":{"id":"ayGLszCqzzoK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The next step is to vectorize our reviews. In the [_Neural Network Foundations_](https://github.com/futuremojo/nlp-demystified/blob/main/notebooks/nlpdemystified_neural_networks_foundations.ipynb) notebook, we used the *texts_to_matrix* method to turn text into binary bags of words.<br><br>\n","Here, we're going to use the *text_to_sequences* method to turn each review into a sequence of integers, with each integer representing its corresponding token.<br>\n","https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#texts_to_sequences\n","\n"],"metadata":{"id":"v5zDD6RCBzxH"}},{"cell_type":"code","source":["%%time\n","X_train = tokenizer.texts_to_sequences(train_reviews)"],"metadata":{"id":"OAd431jp6Ydg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The first review in the training set, vectorized.\n","print(X_train[0])"],"metadata":{"id":"PPo0mNRB6YZL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can look up the corresponding tokens using the tokenizer's *index_word* dict. Here are the tokens corresponding to the first three integers from the first vectorized review."],"metadata":{"id":"kH4I4SeoqVDn"}},{"cell_type":"code","source":["[tokenizer.index_word[x] for x in X_train[0][:3]]"],"metadata":{"id":"jkOI7jYFqb44"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can also convert the integer sequence back to text using the *sequences_to_texts* method, and compare it against the original text.<br>\n","https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#sequences_to_texts"],"metadata":{"id":"ahgfyZhDrHDJ"}},{"cell_type":"code","source":["# Review excerpt reconstructed from integer sequence.\n","tokenizer.sequences_to_texts([X_train[0]])[0][:300]"],"metadata":{"id":"RPlvLdz8rGZZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Original review text.\n","train_reviews.iloc[0][:300]"],"metadata":{"id":"ZirHR4_OC87H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Some models and situations require us to **pad** our sequences to the same length. While that's not the case here, it can still be beneficial to have all our inputs (and consequently, our batches) to be of uniform size to help with optimizations.<br><br>\n","In this case, we'll make all our reviews 200 tokens in length (in practice, you can choose a number based on some analysis). So the reviews longer than 200 tokens will be truncated, while the reviews shorter than 200 will be padded with zeroes.<br>\n","https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences"],"metadata":{"id":"bEqDNjactNk1"}},{"cell_type":"code","source":["MAX_REVIEW_LEN = 200\n","X_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=MAX_REVIEW_LEN)"],"metadata":{"id":"2lTEjMgDqIPp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X_train[0])\n","print(X_train[1])"],"metadata":{"id":"d2tHPfAVv37k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Our training set is prepared. We can now also vectorize and pad our validation set."],"metadata":{"id":"nBAg3laKwDWO"}},{"cell_type":"code","source":["X_val = tokenizer.texts_to_sequences(val_reviews)\n","X_val = keras.preprocessing.sequence.pad_sequences(X_val, maxlen=MAX_REVIEW_LEN)"],"metadata":{"id":"E9P72JJYHqKp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# + 1 to account for padding token.\n","num_tokens = len(tokenizer.word_index) + 1"],"metadata":{"id":"Iw25Irs4J-JG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["we will start with a **random** embedding matrix and let the model come up with its own vectors simultaneously while fitting the training data.<br><br>"],"metadata":{"id":"mBeHd39-sYPS"}},{"cell_type":"code","source":["tf.random.set_seed(0)\n","\n","model = keras.Sequential()\n","\n","# The 'trainable' property is True by default.\n","model.add(layers.Embedding(input_dim=num_tokens,\n","                           output_dim=embedding_dim,\n","                           input_length=MAX_REVIEW_LEN))\n","\n","\n","model.add(layers.GlobalAveragePooling1D())\n","model.add(layers.Dense(128, activation='relu', kernel_initializer=tf.keras.initializers.random_normal(seed=1)))\n","model.add(layers.Dense(64, activation='relu', kernel_initializer=tf.keras.initializers.random_normal(seed=1)))\n","model.add(layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.random_normal(seed=1)))\n","\n","model.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n","history = model.fit(X_train, y_train, epochs=20, batch_size=512, validation_data=(X_val, y_val), callbacks=[es_callback])"],"metadata":{"id":"s6hohIqusa60"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_train_vs_val_performance(history):\n","  training_losses = history.history['loss']\n","  validation_losses = history.history['val_loss']\n","\n","  training_accuracy = history.history['accuracy']\n","  validation_accuracy = history.history['val_accuracy']\n","\n","  epochs = range(1, len(training_losses) + 1)\n","\n","  import matplotlib.pyplot as plt\n","  fig, (ax1, ax2) = plt.subplots(2)\n","  fig.set_figheight(15)\n","  fig.set_figwidth(15)\n","  fig.tight_layout(pad=5.0)\n","\n","  # Plot training vs. validation loss.\n","  ax1.plot(epochs, training_losses, 'bo', label='Training Loss')\n","  ax1.plot(epochs, validation_losses, 'b', label='Validation Loss')\n","  ax1.title.set_text('Training vs. Validation Loss')\n","  ax1.set_xlabel('Epoch')\n","  ax1.set_ylabel('Loss')\n","  ax1.legend()\n","\n","  # PLot training vs. validation accuracy.\n","  ax2.plot(epochs, training_accuracy, 'bo', label='Training Accuracy')\n","  ax2.plot(epochs, validation_accuracy, 'b', label='Validation Accuracy')\n","  ax2.title.set_text('Training vs. Validation Accuracy')\n","  ax2.set_xlabel('Epoch')\n","  ax2.set_ylabel('Accuracy')\n","  ax2.legend()\n","\n","  plt.show()"],"metadata":{"id":"QiZqZH3ztV6Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_train_vs_val_performance(history)"],"metadata":{"id":"ZKyZovd5tWL_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf.random.set_seed(0)\n","\n","model = keras.Sequential()\n","\n","# The 'trainable' property is True by default.\n","model.add(layers.Embedding(input_dim=num_tokens,\n","                           output_dim=embedding_dim,\n","                           input_length=MAX_REVIEW_LEN))\n","\n","\n","model.add(layers.GlobalAveragePooling1D())\n","model.add(layers.Dense(128, activation='relu', kernel_initializer=tf.keras.initializers.random_normal(seed=1)))\n","model.add(layers.Dense(64, activation='relu', kernel_initializer=tf.keras.initializers.random_normal(seed=1)))\n","model.add(layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.random_normal(seed=1)))\n","\n","model.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","model.fit(X_train, y_train, epochs=4, batch_size=512, validation_data=(X_val, y_val))"],"metadata":{"id":"ynr_-3E3tWPC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["yelp_test = pd.read_csv('yelp_review_polarity_csv/test.csv', names=['sentiment', 'review'])"],"metadata":{"id":"AUsripS7SZdk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["yelp_test['sentiment'].replace(to_replace=1, value=0, inplace=True)\n","yelp_test['sentiment'].replace(to_replace=2, value=1, inplace=True)\n","yelp_test.head()"],"metadata":{"id":"bm4Day_jYbTe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_test = np.array(yelp_test['sentiment'])\n","print(y_test)"],"metadata":{"id":"Qks0uXyzYbTe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_test = tokenizer.texts_to_sequences(yelp_test['review'])"],"metadata":{"id":"-UyBts6EYbTe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=MAX_REVIEW_LEN)"],"metadata":{"id":"loh2rh3WYbTe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.evaluate(X_test, y_test)"],"metadata":{"id":"dvCk3eQDsa9j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def sentiment(reviews):\n","  seqs = tokenizer.texts_to_sequences(reviews)\n","  seqs = keras.preprocessing.sequence.pad_sequences(seqs, maxlen=MAX_REVIEW_LEN)\n","  return model.predict(seqs)\n"],"metadata":{"id":"rNbNyr8PVJMv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Real reviews from Google Reviews.\n","pos_review = \"The best seafood joint in East Village San Diego!  Great lobster roll, great fish, great oysters, great bread, great cocktails, and such amazing service.  The atmosphere is top notch and the location is so much fun being located just a block away from Petco Park (San Diego Padres Stadium).\"\n","neg_review = \"A thoroughly disappointing experience. When you book a Marriott you expect a certain standard. Albany falls way short. Room cleaning has to be booked 24 hours in advance but nobody thought to mention this at check in. The hotel is tired and needs a face-lift. The only bright light in a sea of mediocrity were the pancakes at breakfast. Sadly they weren't enough to save the experience. If you travel to Albany, then do yourself a big favour and book the Westin.\""],"metadata":{"id":"Jy-4MWV1VJRA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(sentiment([pos_review, neg_review]))"],"metadata":{"id":"wH8b-Z3GVJU3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We get correct sentiment prediction by the model."],"metadata":{"id":"Wxl5bYGDvLET"}}]}